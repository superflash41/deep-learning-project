\section{Model Training}
\label{sec:model-training}

In this section, we discuss how we trained each of our three CNNs (DenseNet, ResNet,
and Xception) after determining optimal hyperparameters using a systematic search phase.

\subsection{Hyperparameter Selection}
\label{subsec:hyperparameter-selection}

We explored several hyperparameters, focusing on those that strongly affect model capacity
and convergence:

\begin{itemize}
    \item \textbf{Learning Rate (LR):} Controls the step size in gradient descent updates.
    Too high can cause divergence; too low can slow training.
    \item \textbf{Dropout Rate:} Randomly deactivates neurons during training to combat
    overfitting. We tested rates from 0.2 to 0.5.
    \item \textbf{L2 Regularization Factor:} Encourages smaller weight magnitudes to reduce
    overfitting. We examined values in \(\{1 \times 10^{-4}, 5 \times 10^{-4}, 1 \times 10^{-3}\}\).
    \item \textbf{Number of Unfrozen Layers:} Determines how much of the pretrained backbone
    is fine-tuned. For each model, we tested unfreezing anywhere from 5 to 45 layers, depending
    on the architecture.
\end{itemize}

These hyperparameters were targeted because they strongly influence both the representation
power of the model and its generalization
Unfreezing more layers can extract specialized features, though it requires more computational effort and data.
Meanwhile, a suitable dropout and L2 factor can markedly reduce overfitting in smaller or imbalanced datasets.

\subsection{Hyperparameter Search Phase}
\label{subsec:hyperparameter-search-phase}

To find the optimal configuration for each network, we used \textbf{Keras Tuner} with
a Random Search strategy:
\begin{itemize}
    \item \textbf{Trials:} Up to 10 trials per model.
    \item \textbf{Executions per trial:} 1 or 2, to confirm reproducibility.
    \item \textbf{Objective:} Maximize validation accuracy on the FLAME dataset.
\end{itemize}

During this search phase:
\begin{enumerate}
    \item We loaded each model (DenseNet121, ResNet152, Xception) with ImageNet weights
    frozen.
    \item Keras Tuner varied learning rate, dropout, L2 factor, and number of unfrozen layers.
    \item Each candidate model was trained for up to 20 epochs (with early stopping), and we
    tracked validation loss/accuracy.
    \item The best-performing hyperparameters were then saved and used for final training.
\end{enumerate}

\subsection{Final Training Procedure}
\label{subsec:final-training-procedure}

After identifying the best hyperparameters, we performed the \textbf{final training} step
as follows:

\begin{enumerate}
    \item \textbf{Model Initialization:} We loaded each pretrained backbone (DenseNet121,
    ResNet152, or Xception) and unfroze the exact number of layers determined by the tuner.
    \item \textbf{Compile and Train:} We compiled with Adam, using the selected learning
    rate and including dropout and L2 factors. We trained each model on the augmented FLAME
    training set, plus a validation split, for up to 30 epochs. Early stopping prevented
    overfitting.
    \item \textbf{Checkpointing:} We saved the best epochâ€™s weights as \texttt{model\_final.keras}.
\end{enumerate}

Below is a summary of the final hyperparameters for each model:

\begin{itemize}
    \item \textbf{Xception:}
    \begin{itemize}
       \item Unfrozen Layers: 25
       \item Dropout: 0.45
       \item L2 Factor: 0.001
       \item Learning Rate: 0.00541
    \end{itemize}
    \item \textbf{DenseNet121:}
    \begin{itemize}
       \item Unfrozen Layers: 20
       \item Dropout: 0.35
       \item L2 Factor: 0.001
       \item Learning Rate: 0.00147
    \end{itemize}
    \item \textbf{ResNet152:}
    \begin{itemize}
       \item Unfrozen Layers: 45
       \item Dropout: 0.40
       \item L2 Factor: 0.0005
       \item Learning Rate: 0.00093
    \end{itemize}
\end{itemize}

These settings allowed each model to adapt to wildfire detection with good stability and minimal overfitting.
The next step is to combine the individually trained models in an ensemble approach, as described in later sections.
